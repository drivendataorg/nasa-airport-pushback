{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc035655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48d0b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model data will be saved at: Models/6/\n"
     ]
    }
   ],
   "source": [
    "# Define thresholds and airport lists\n",
    "threshold_ml_classifier_model = .50\n",
    "mae_thresh_bad = 30\n",
    "mae_thresh_good = 20\n",
    "bad_airports = [\"KDFW\", \"KJFK\", \"KMEM\", \"KMIA\"]\n",
    "good_airports = [\"KATL\", \"KCLT\", \"KDEN\", \"KORD\", \"KPHX\", \"KSEA\"]\n",
    "\n",
    "# Set comments and model type\n",
    "comment = \"submission etd airlinecode taxitime\"\n",
    "model_type = \"xgb classifier\"\n",
    "\n",
    "# Set directories for loading and saving data\n",
    "raw_label_load_dir = \"Data/\"\n",
    "indiv_features_load_dir = f\"Training_Extracted_Features/Current_Features/\"\n",
    "\n",
    "unique_timepoint_features = [\"taxitime_to_gate\"]\n",
    "timepoint_root = f\"{indiv_features_load_dir}timepoint\"\n",
    "\n",
    "unique_gufi_features = [\"airlinecode\"]\n",
    "gufi_root = f\"{indiv_features_load_dir}gufi\"\n",
    "\n",
    "\n",
    "unique_timepointgufi_features = [\"etd\"]\n",
    "timepointgufi_root = f\"{indiv_features_load_dir}timepointgufi\"\n",
    "\n",
    "\n",
    "save_dir = f\"Models/\"\n",
    "\n",
    "# Set file paths for submission data\n",
    "indiv_features_load_dir_submission = f\"Indiv Engineered Features/ToCombineSubmission/\"\n",
    "timepoint_root_submission = f\"{indiv_features_load_dir_submission}timepoint\"\n",
    "gufi_root_submission = f\"{indiv_features_load_dir_submission}gufi\"\n",
    "timepointgufi_root_submission = f\"{indiv_features_load_dir_submission}timepointgufi\"\n",
    "\n",
    "# Set headers for the prediction output file\n",
    "header = [\"gufi\", \"timestamp\", \"airport\", \"minutes_until_pushback\"]\n",
    "\n",
    "# Create the directory for saving the model and the prediction output file\n",
    "run_id = 0\n",
    "model_save_dir = f\"{save_dir}{run_id}/\"\n",
    "while os.path.isdir(model_save_dir):\n",
    "    run_id = run_id + 1\n",
    "    model_save_dir = f\"{save_dir}{run_id}/\"\n",
    "os.mkdir(model_save_dir)\n",
    "print(f\"Model data will be saved at: {model_save_dir}\")\n",
    "\n",
    "# Initialize the overall prediction output file with headers\n",
    "overall_prediction_file_name_submission = f\"{model_save_dir}overall_submission.csv\"\n",
    "with open(overall_prediction_file_name_submission, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "# Function to split the 'gufi' field into separate columns\n",
    "def split_gufi(curr_df):\n",
    "    curr_df[['plane_id', 'departing_airport_code', 'arriving_airport_code']] = curr_df.gufi.str.split('.', expand=True)[[0, 1, 2]]\n",
    "    curr_df['airline_code'] = curr_df.gufi.str[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee8ad28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Doing airport: KATL\n",
      "Loaded training file from : Data/KATL/train_labels_KATL.csv\n",
      "Loaded etd features from from : Training_Extracted_Features/Current Features/timepointgufi_KATL_etd.csv\n",
      "Loaded airlinecode features from from : Training_Extracted_Features/Current Features/gufi_KATL_airlinecode.csv\n",
      "Loaded taxitime to gate features from from : Training_Extracted_Features/Current Features/timepoint_KATL_taxitime_to_gate.csv\n",
      "Internal Regressor Len of Training: 1252252\n",
      "Internal Regressor Len of Prediction: 1881193\n",
      "MAE: 9.841572874234595 & MSE: 330.58379336942033\n",
      "Only 185511 out of 1881193 were above the threshold of 20 and were removed from training.\n",
      "Trained on 1695682 many datapoints.\n",
      "Number of Overestimated Samples: 1106941 & Number of Underestimated Samples: 774252\n",
      "-----------------------------\n",
      "Doing airport: KCLT\n",
      "Loaded training file from : Data/KCLT/train_labels_KCLT.csv\n",
      "Loaded etd features from from : Training_Extracted_Features/Current Features/timepointgufi_KCLT_etd.csv\n",
      "Loaded airlinecode features from from : Training_Extracted_Features/Current Features/gufi_KCLT_airlinecode.csv\n",
      "Loaded taxitime to gate features from from : Training_Extracted_Features/Current Features/timepoint_KCLT_taxitime_to_gate.csv\n",
      "Internal Regressor Len of Training: 832253\n",
      "Internal Regressor Len of Prediction: 1249454\n",
      "MAE: 11.110124902557438 & MSE: 423.3549790548512\n",
      "Only 138538 out of 1249454 were above the threshold of 20 and were removed from training.\n",
      "Trained on 1110916 many datapoints.\n",
      "Number of Overestimated Samples: 733290 & Number of Underestimated Samples: 516164\n",
      "-----------------------------\n",
      "Doing airport: KDEN\n",
      "Loaded training file from : Data/KDEN/train_labels_KDEN.csv\n",
      "Loaded etd features from from : Training_Extracted_Features/Current Features/timepointgufi_KDEN_etd.csv\n",
      "Loaded airlinecode features from from : Training_Extracted_Features/Current Features/gufi_KDEN_airlinecode.csv\n",
      "Loaded taxitime to gate features from from : Training_Extracted_Features/Current Features/timepoint_KDEN_taxitime_to_gate.csv\n",
      "Internal Regressor Len of Training: 1114507\n",
      "Internal Regressor Len of Prediction: 1670766\n",
      "MAE: 11.563950307822878 & MSE: 406.17684762558014\n",
      "Only 193370 out of 1670766 were above the threshold of 20 and were removed from training.\n",
      "Trained on 1477396 many datapoints.\n",
      "Number of Overestimated Samples: 964414 & Number of Underestimated Samples: 706352\n",
      "-----------------------------\n",
      "Doing airport: KDFW\n",
      "Loaded training file from : Data/KDFW/train_labels_KDFW.csv\n",
      "Loaded etd features from from : Training_Extracted_Features/Current Features/timepointgufi_KDFW_etd.csv\n",
      "Loaded airlinecode features from from : Training_Extracted_Features/Current Features/gufi_KDFW_airlinecode.csv\n",
      "Loaded taxitime to gate features from from : Training_Extracted_Features/Current Features/timepoint_KDFW_taxitime_to_gate.csv\n",
      "Internal Regressor Len of Training: 1213790\n",
      "Internal Regressor Len of Prediction: 1822318\n",
      "MAE: 13.354624165485935 & MSE: 544.5007150233932\n",
      "Only 145162 out of 1822318 were above the threshold of 30 and were removed from training.\n",
      "Trained on 1677156 many datapoints.\n",
      "Number of Overestimated Samples: 1123825 & Number of Underestimated Samples: 698493\n",
      "-----------------------------\n",
      "Doing airport: KJFK\n",
      "Loaded training file from : Data/KJFK/train_labels_KJFK.csv\n",
      "Loaded etd features from from : Training_Extracted_Features/Current Features/timepointgufi_KJFK_etd.csv\n",
      "Loaded airlinecode features from from : Training_Extracted_Features/Current Features/gufi_KJFK_airlinecode.csv\n",
      "Loaded taxitime to gate features from from : Training_Extracted_Features/Current Features/timepoint_KJFK_taxitime_to_gate.csv\n",
      "Internal Regressor Len of Training: 480676\n",
      "Internal Regressor Len of Prediction: 720226\n",
      "MAE: 14.65148439517596 & MSE: 625.7930455162685\n",
      "Only 70336 out of 720226 were above the threshold of 30 and were removed from training.\n",
      "Trained on 649890 many datapoints.\n",
      "Number of Overestimated Samples: 415264 & Number of Underestimated Samples: 304962\n",
      "-----------------------------\n",
      "Doing airport: KMEM\n",
      "Loaded training file from : Data/KMEM/train_labels_KMEM.csv\n",
      "Loaded etd features from from : Training_Extracted_Features/Current Features/timepointgufi_KMEM_etd.csv\n",
      "Loaded airlinecode features from from : Training_Extracted_Features/Current Features/gufi_KMEM_airlinecode.csv\n",
      "Loaded taxitime to gate features from from : Training_Extracted_Features/Current Features/timepoint_KMEM_taxitime_to_gate.csv\n",
      "Internal Regressor Len of Training: 449377\n",
      "Internal Regressor Len of Prediction: 675370\n",
      "MAE: 17.468414350652235 & MSE: 693.084432237144\n",
      "Only 104181 out of 675370 were above the threshold of 30 and were removed from training.\n",
      "Trained on 571189 many datapoints.\n",
      "Number of Overestimated Samples: 365282 & Number of Underestimated Samples: 310088\n",
      "-----------------------------\n",
      "Doing airport: KMIA\n",
      "Loaded training file from : Data/KMIA/train_labels_KMIA.csv\n",
      "Loaded etd features from from : Training_Extracted_Features/Current Features/timepointgufi_KMIA_etd.csv\n",
      "Loaded airlinecode features from from : Training_Extracted_Features/Current Features/gufi_KMIA_airlinecode.csv\n",
      "Loaded taxitime to gate features from from : Training_Extracted_Features/Current Features/timepoint_KMIA_taxitime_to_gate.csv\n",
      "Internal Regressor Len of Training: 549025\n",
      "Internal Regressor Len of Prediction: 823744\n",
      "MAE: 14.269026542226712 & MSE: 605.0222022861471\n",
      "Only 74377 out of 823744 were above the threshold of 30 and were removed from training.\n",
      "Trained on 749367 many datapoints.\n",
      "Number of Overestimated Samples: 485969 & Number of Underestimated Samples: 337775\n",
      "-----------------------------\n",
      "Doing airport: KORD\n",
      "Loaded training file from : Data/KORD/train_labels_KORD.csv\n",
      "Loaded etd features from from : Training_Extracted_Features/Current Features/timepointgufi_KORD_etd.csv\n",
      "Loaded airlinecode features from from : Training_Extracted_Features/Current Features/gufi_KORD_airlinecode.csv\n",
      "Loaded taxitime to gate features from from : Training_Extracted_Features/Current Features/timepoint_KORD_taxitime_to_gate.csv\n",
      "Internal Regressor Len of Training: 1144369\n",
      "Internal Regressor Len of Prediction: 1714842\n",
      "MAE: 11.798962819898277 & MSE: 465.8121243823046\n",
      "Only 201867 out of 1714842 were above the threshold of 20 and were removed from training.\n",
      "Trained on 1512975 many datapoints.\n",
      "Number of Overestimated Samples: 1005337 & Number of Underestimated Samples: 709505\n",
      "-----------------------------\n",
      "Doing airport: KPHX\n",
      "Loaded training file from : Data/KPHX/train_labels_KPHX.csv\n",
      "Loaded etd features from from : Training_Extracted_Features/Current Features/timepointgufi_KPHX_etd.csv\n",
      "Loaded airlinecode features from from : Training_Extracted_Features/Current Features/gufi_KPHX_airlinecode.csv\n",
      "Loaded taxitime to gate features from from : Training_Extracted_Features/Current Features/timepoint_KPHX_taxitime_to_gate.csv\n",
      "Internal Regressor Len of Training: 659501\n",
      "Internal Regressor Len of Prediction: 991075\n",
      "MAE: 10.13960598340186 & MSE: 354.6142844890649\n",
      "Only 92625 out of 991075 were above the threshold of 20 and were removed from training.\n",
      "Trained on 898450 many datapoints.\n",
      "Number of Overestimated Samples: 576141 & Number of Underestimated Samples: 414934\n",
      "-----------------------------\n",
      "Doing airport: KSEA\n",
      "Loaded training file from : Data/KSEA/train_labels_KSEA.csv\n",
      "Loaded etd features from from : Training_Extracted_Features/Current Features/timepointgufi_KSEA_etd.csv\n",
      "Loaded airlinecode features from from : Training_Extracted_Features/Current Features/gufi_KSEA_airlinecode.csv\n",
      "Loaded taxitime to gate features from from : Training_Extracted_Features/Current Features/timepoint_KSEA_taxitime_to_gate.csv\n",
      "Internal Regressor Len of Training: 648009\n",
      "Internal Regressor Len of Prediction: 969834\n",
      "MAE: 8.51022339905592 & MSE: 281.1777994997082\n",
      "Only 76215 out of 969834 were above the threshold of 20 and were removed from training.\n",
      "Trained on 893619 many datapoints.\n",
      "Number of Overestimated Samples: 585871 & Number of Underestimated Samples: 383963\n"
     ]
    }
   ],
   "source": [
    "list_airports = [\"KATL\", \"KCLT\", \"KDEN\", \"KDFW\", \"KJFK\", \"KMEM\", \"KMIA\", \"KORD\", \"KPHX\", \"KSEA\"]\n",
    "\n",
    "# Iterate through each airport in the list_airports\n",
    "for airport in list_airports:\n",
    "    print(f'-----------------------------')\n",
    "    print(f'Doing airport: {airport}')\n",
    "    feature_cols = ['unix_time']    \n",
    "\n",
    "    # 1. Load training data and feature files\n",
    "    raw_labels_load_file = f\"{raw_label_load_dir}{airport}/train_labels_{airport}.csv\"\n",
    "    df_data = pd.read_csv(raw_labels_load_file, parse_dates=[\"timestamp\"])\n",
    "    print(f\"Loaded training file from : {raw_labels_load_file}\")\n",
    "\n",
    "    # Load ETD, airlinecode, and taxitime_to_gate features\n",
    "    # Merge them with the main dataframe (df_data) using appropriate keys\n",
    "\n",
    "    ###ETD\n",
    "    etd_file_path = f\"{timepointgufi_root}_{airport}_etd.csv\"\n",
    "    df_etd = pd.read_csv(etd_file_path, parse_dates=[\"timestamp\"])\n",
    "    print(f\"Loaded etd features from from : {etd_file_path}\")\n",
    "    df_data = pd.merge(df_data, df_etd, on=['gufi', 'timestamp'])\n",
    "    etd_features = ['minutes_until_departure_from_timepoint', 'minutes_until_departure_from_timestamp', 'mean_departure_from_timepoint', 'std_departure_from_timepoint']\n",
    "    feature_cols.extend(etd_features)\n",
    "\n",
    "    ###airlinecode\n",
    "    airlinecode_file_path = f\"{gufi_root}_{airport}_airlinecode.csv\"\n",
    "    df_airlinecode = pd.read_csv(airlinecode_file_path)\n",
    "    print(f\"Loaded airlinecode features from from : {airlinecode_file_path}\")\n",
    "    df_data = df_data.drop(columns=['airport'])\n",
    "    df_data = pd.merge(df_data, df_airlinecode, on=['gufi'], how=\"left\")\n",
    "    airlinecode_features = list(df_airlinecode.columns)\n",
    "    airlinecode_features.remove('gufi')\n",
    "    airlinecode_features.remove('airport')\n",
    "    feature_cols.extend(airlinecode_features)\n",
    "\n",
    "    ###taxitimetogate\n",
    "    taxitime_to_gate_file_path = f\"{timepoint_root}_{airport}_taxitime_to_gate.csv\"\n",
    "    df_taxitime_to_gate = pd.read_csv(taxitime_to_gate_file_path, parse_dates=[\"timestamp\"])\n",
    "    print(f\"Loaded taxitime to gate features from from : {taxitime_to_gate_file_path}\")\n",
    "    df_data = pd.merge(df_data, df_taxitime_to_gate, on=['timestamp'], how=\"left\")\n",
    "    taxitime_to_gate_features = list(df_taxitime_to_gate.columns)\n",
    "    if \"Unnamed: 0\" in list(taxitime_to_gate_features):\n",
    "        taxitime_to_gate_features.remove(\"Unnamed: 0\")\n",
    "    taxitime_to_gate_features.remove('found_counts_taxitime_to_gate')\n",
    "    taxitime_to_gate_features.remove('timestamp')\n",
    "    feature_cols.extend(taxitime_to_gate_features)\n",
    "\n",
    "    # 2. Preprocess the data\n",
    "    # - Extract year, month, day, and hour information from timestamp\n",
    "    # - Drop unnecessary columns and handle missing values\n",
    "\n",
    "    if \"Unnamed: 0\" in list(df_data):\n",
    "        df_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    # extract year, month, day, and hour information\n",
    "    df_data['unix_time'] = df_data['timestamp'].astype(np.int64)  // 10**9\n",
    "    df_data.dropna(inplace=True)\n",
    "    df_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    label_col = ['minutes_until_pushback']\n",
    "    to_save_dataset = ['gufi','timestamp','airport']\n",
    "    to_save_dataset.extend(list(set(feature_cols)))\n",
    "    df_data_used = df_data[to_save_dataset]\n",
    "\n",
    "    # 3. Split the data into train and test sets using GroupShuffleSplit\n",
    "    # - Create an internal regressor model and fit it to the training data\n",
    "    # - Make predictions on the test data and calculate MAE and MSE\n",
    "\n",
    "    # 4. Filter the data based on a threshold for MAE\n",
    "    # - Train a lower model (regressor_lower) on the filtered data\n",
    "\n",
    "    # 5. Save the lower model as a pickle file\n",
    "\n",
    "    # 6. Train an estimation classifier to predict overestimation or underestimation\n",
    "    # - Calculate the median underestimation and overestimation\n",
    "    # - Save the classifier and its parameters as pickle files\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=0.4, test_size=0.6, random_state=42)\n",
    "    # split the data into train and test sets\n",
    "    train_index, test_index = next(gss.split(df_data, groups=df_data['gufi']))\n",
    "    df_internal_train = df_data.iloc[train_index].copy()\n",
    "    df_internal_test = df_data.iloc[test_index].copy()\n",
    "\n",
    "    X_internal_train = df_internal_train[list(set(feature_cols))]\n",
    "    y_internal_train = df_internal_train[label_col]\n",
    "\n",
    "    X_internal_test = df_internal_test[list(set(feature_cols))]\n",
    "    y_internal_test = df_internal_test[label_col]\n",
    "    internal_regressor = xgb.XGBRegressor()  \n",
    "    print(f\"Internal Regressor Len of Training: {len(X_internal_train)}\")\n",
    "    internal_regressor.fit(X_internal_train, y_internal_train)  \n",
    "    print(f\"Internal Regressor Len of Prediction: {len(X_internal_test)}\")\n",
    "    y_internal_pred = np.int32(np.around(internal_regressor.predict(X_internal_test),decimals=0))\n",
    "    internal_mae = mean_absolute_error(y_internal_test, y_internal_pred)\n",
    "    internal_mse = mean_squared_error(y_internal_test, y_internal_pred)\n",
    "    # # Print the results\n",
    "    print(f\"MAE: {internal_mae} & MSE: {internal_mse}\")  \n",
    "    df_internal_test['y_pred'] = y_internal_pred\n",
    "    df_internal_test['mae'] = np.abs(df_internal_test['y_pred'] - df_internal_test['minutes_until_pushback'])\n",
    "    df_internal_test['ml_model_used'] = 0\n",
    "    if airport in bad_airports:\n",
    "        mae_thresh = mae_thresh_bad\n",
    "    else:\n",
    "        mae_thresh = mae_thresh_good\n",
    "    bool_mask = (df_internal_test['mae']  > mae_thresh)\n",
    "    print(f'Only {sum(bool_mask)} out of {len(df_internal_test)} were above the threshold of {mae_thresh} and were removed from training.')\n",
    "    df_internal_test.loc[bool_mask,'ml_model_used'] = 1\n",
    "    df_internal_test_lower = df_internal_test[df_internal_test.ml_model_used == 0].copy()\n",
    "    df_internal_test_higher = df_internal_test[df_internal_test.ml_model_used == 1].copy()\n",
    "    \n",
    "    X_lower = df_internal_test_lower[list(set(feature_cols))]\n",
    "    y_lower = df_internal_test_lower[label_col]\n",
    " \n",
    "    regressor_lower = xgb.XGBRegressor()  \n",
    "    print(f'Trained on {len(X_lower)} many datapoints.')\n",
    "    regressor_lower.fit(X_lower, y_lower) \n",
    "    ##########\n",
    "    \n",
    "    model_file_name = f\"{model_save_dir}{airport}_{model_type}_NOOUTLIER_{comment}.pkl\"\n",
    "    pickle.dump(regressor_lower, open(model_file_name, \"wb\"))\n",
    "\n",
    "\n",
    "    # Training Over and Under Estimation Classifier\n",
    "    classifier_upper_threshold = 0.50\n",
    "    classifier_lower_threshold = 0.50\n",
    "    y_pred = np.int32(np.around(regressor_lower.predict(df_internal_test[list(set(feature_cols))]),decimals=0))   \n",
    "\n",
    "    X_test = df_internal_test.copy(deep=True)\n",
    "    X_test['pred_minutes_until_pushback'] = y_pred\n",
    "    X_test['actual_minutes_until_pushback'] = X_test['minutes_until_pushback']\n",
    "    X_test['label_estimation'] = 0\n",
    "    X_test.loc[X_test['pred_minutes_until_pushback'] < X_test['actual_minutes_until_pushback'], 'label_estimation'] = 1\n",
    "    num_over_samples = len(X_test[X_test['label_estimation'] == 0])\n",
    "    num_under_samples = len(X_test[X_test['label_estimation'] == 1])\n",
    "    print(f'Number of Overestimated Samples: {num_over_samples} & Number of Underestimated Samples: {num_under_samples}')\n",
    "    estimate_classifier = xgb.XGBClassifier()\n",
    "    estimate_classifier.fit(X_test[list(set(feature_cols)) + ['pred_minutes_until_pushback']], X_test['label_estimation'])\n",
    "    y_prob_estimate = estimate_classifier.predict_proba(X_test[list(set(feature_cols)) + ['pred_minutes_until_pushback']])[:, 1]\n",
    "    y_pred_estimate = np.int32(np.around(estimate_classifier.predict(X_test[list(set(feature_cols)) + ['pred_minutes_until_pushback']]),decimals=0)) \n",
    "    test_y_binary = X_test['label_estimation']    \n",
    "    df_test_estimate_underestimate = X_test[y_prob_estimate > classifier_upper_threshold]\n",
    "    median_underestimation = (df_test_estimate_underestimate['actual_minutes_until_pushback'] - df_test_estimate_underestimate['pred_minutes_until_pushback']).median()\n",
    "    df_test_estimate_overestimate = X_test[y_prob_estimate < classifier_lower_threshold]\n",
    "    median_overestimation = (df_test_estimate_overestimate['pred_minutes_until_pushback'] - df_test_estimate_overestimate['actual_minutes_until_pushback']).median()\n",
    "\n",
    "    # Saving classifier\n",
    "    model_file_name = f\"{model_save_dir}{airport}_estimation_classifier.pkl\"\n",
    "    pickle.dump(estimate_classifier, open(model_file_name, \"wb\"))\n",
    "    airport_estimation_param_dict = {}\n",
    "    airport_estimation_param_dict['median_underestimation'] = median_underestimation\n",
    "    airport_estimation_param_dict['median_overestimation'] = median_overestimation\n",
    "    params_file_name = f\"{model_save_dir}{airport}_estimation_parameters.pkl\"\n",
    "    pickle.dump(airport_estimation_param_dict, open(params_file_name, \"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d39ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pushback_plane_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ceb0af7f7076ef9810a960c24b1745278bfec469964610e4b4c66a5928333a0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
