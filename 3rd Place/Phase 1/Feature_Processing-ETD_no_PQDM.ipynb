{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60b72af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.options.mode.chained_assignment = None\n",
    "from helper import extract_etdv3, data_loader_etd, data_loader_submission_etd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b315987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the following dir: Inference_Extracted_Features/etd_1682865453.7603784/\n"
     ]
    }
   ],
   "source": [
    "bool_submission_prep = 1\n",
    "\n",
    "# Set the save directory based on whether we are preparing for submission or not\n",
    "if bool_submission_prep:\n",
    "    sav_dir = f\"Inference_Extracted_Features/etd_{time.time()}/\"    \n",
    "else:\n",
    "    sav_dir = f\"Training_Extracted_Features/etd_{time.time()}/\"\n",
    "\n",
    "# Define the directory path for loading data\n",
    "load_dir = \"Data/\"\n",
    "\n",
    "# Create the save directory\n",
    "os.mkdir(f\"{sav_dir}\")\n",
    "\n",
    "# Print the created directory path\n",
    "print(f'Created the following dir: {sav_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b64e61d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Doing airport: KATL\n",
      "Loading in dataframes for: KATL\n",
      "LOading from: Data/submission_data.csv\n",
      "Feature Engineering for: KATL doing unique timestamp\n",
      "There were 0 gufis from train labels because they werent in etd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 50164/50165 [04:40<00:00, 178.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the following file: Inference_Extracted_Features/etd_1682865453.7603784/timepointgufi_KATL_etd.csv\n",
      "-----------------------------\n",
      "Doing airport: KCLT\n",
      "Loading in dataframes for: KCLT\n",
      "LOading from: Data/submission_data.csv\n",
      "Feature Engineering for: KCLT doing unique timestamp\n",
      "There were 0 gufis from train labels because they werent in etd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 35707/35708 [03:11<00:00, 186.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the following file: Inference_Extracted_Features/etd_1682865453.7603784/timepointgufi_KCLT_etd.csv\n",
      "-----------------------------\n",
      "Doing airport: KDEN\n",
      "Loading in dataframes for: KDEN\n",
      "LOading from: Data/submission_data.csv\n",
      "Feature Engineering for: KDEN doing unique timestamp\n",
      "There were 0 gufis from train labels because they werent in etd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 42521/42522 [04:23<00:00, 161.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the following file: Inference_Extracted_Features/etd_1682865453.7603784/timepointgufi_KDEN_etd.csv\n",
      "-----------------------------\n",
      "Doing airport: KDFW\n",
      "Loading in dataframes for: KDFW\n",
      "LOading from: Data/submission_data.csv\n",
      "Feature Engineering for: KDFW doing unique timestamp\n",
      "There were 0 gufis from train labels because they werent in etd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 46278/46279 [04:40<00:00, 165.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the following file: Inference_Extracted_Features/etd_1682865453.7603784/timepointgufi_KDFW_etd.csv\n",
      "-----------------------------\n",
      "Doing airport: KJFK\n",
      "Loading in dataframes for: KJFK\n",
      "LOading from: Data/submission_data.csv\n",
      "Feature Engineering for: KJFK doing unique timestamp\n",
      "There were 0 gufis from train labels because they werent in etd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 16608/16609 [01:33<00:00, 176.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the following file: Inference_Extracted_Features/etd_1682865453.7603784/timepointgufi_KJFK_etd.csv\n",
      "-----------------------------\n",
      "Doing airport: KMEM\n",
      "Loading in dataframes for: KMEM\n",
      "LOading from: Data/submission_data.csv\n",
      "Feature Engineering for: KMEM doing unique timestamp\n",
      "There were 0 gufis from train labels because they werent in etd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 13859/13860 [01:50<00:00, 125.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the following file: Inference_Extracted_Features/etd_1682865453.7603784/timepointgufi_KMEM_etd.csv\n",
      "-----------------------------\n",
      "Doing airport: KMIA\n",
      "Loading in dataframes for: KMIA\n",
      "LOading from: Data/submission_data.csv\n",
      "Feature Engineering for: KMIA doing unique timestamp\n",
      "There were 0 gufis from train labels because they werent in etd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 20922/20923 [01:59<00:00, 174.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the following file: Inference_Extracted_Features/etd_1682865453.7603784/timepointgufi_KMIA_etd.csv\n",
      "-----------------------------\n",
      "Doing airport: KORD\n",
      "Loading in dataframes for: KORD\n",
      "LOading from: Data/submission_data.csv\n",
      "Feature Engineering for: KORD doing unique timestamp\n",
      "There were 0 gufis from train labels because they werent in etd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 46707/46708 [04:40<00:00, 166.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the following file: Inference_Extracted_Features/etd_1682865453.7603784/timepointgufi_KORD_etd.csv\n",
      "-----------------------------\n",
      "Doing airport: KPHX\n",
      "Loading in dataframes for: KPHX\n",
      "LOading from: Data/submission_data.csv\n",
      "Feature Engineering for: KPHX doing unique timestamp\n",
      "There were 0 gufis from train labels because they werent in etd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 26023/26024 [02:31<00:00, 171.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the following file: Inference_Extracted_Features/etd_1682865453.7603784/timepointgufi_KPHX_etd.csv\n",
      "-----------------------------\n",
      "Doing airport: KSEA\n",
      "Loading in dataframes for: KSEA\n",
      "LOading from: Data/submission_data.csv\n",
      "Feature Engineering for: KSEA doing unique timestamp\n",
      "There were 0 gufis from train labels because they werent in etd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 26139/26140 [02:29<00:00, 175.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the following file: Inference_Extracted_Features/etd_1682865453.7603784/timepointgufi_KSEA_etd.csv\n"
     ]
    }
   ],
   "source": [
    "# List of airports to process\n",
    "list_airports = [\"KATL\", \"KCLT\", \"KDEN\", \"KDFW\", \"KJFK\", \"KMEM\", \"KMIA\", \"KORD\", \"KPHX\", \"KSEA\"]\n",
    "\n",
    "# Process each airport in the list\n",
    "for airport in list_airports:\n",
    "    print(f'-----------------------------')\n",
    "    print(f'Doing airport: {airport}')\n",
    "\n",
    "    # Load data for the current airport\n",
    "    print(f'Loading in dataframes for: {airport}')\n",
    "    if bool_submission_prep:\n",
    "        df_train_labels, df_etd = data_loader_submission_etd(load_dir, airport)\n",
    "    else:\n",
    "        df_train_labels, df_etd = data_loader_etd(load_dir, airport)\n",
    "\n",
    "    # Feature Engineering for unique timestamp\n",
    "    print(f'Feature Engineering for: {airport} doing unique timestamp')\n",
    "    \n",
    "    # Prepare dataframes for processing\n",
    "    df_final_labels_etd = df_train_labels.copy(deep=True)\n",
    "    df_final_labels_etd = df_final_labels_etd.sort_values(by=['gufi', 'timestamp']).reset_index(drop=True)\n",
    "    df_etd_limited = df_etd[df_etd.gufi.isin(df_final_labels_etd.gufi.unique())].sort_values(by=['gufi', 'timestamp']).reset_index(drop=True)\n",
    "    df_final_labels_etd = df_final_labels_etd[df_final_labels_etd.gufi.isin(df_etd.gufi.unique())].sort_values(by=['gufi', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "    # Initialize necessary variables and dataframes\n",
    "    unique_planes = list(df_final_labels_etd.gufi.unique())\n",
    "    df_etd_limited_no_dup = df_etd_limited.drop_duplicates(subset=['gufi'])\n",
    "    new_planes_indexes_etd = list(df_etd_limited_no_dup.index)\n",
    "    dropped_fin_labels = len(df_train_labels) - len(df_final_labels_etd)\n",
    "    print(f'There were {dropped_fin_labels} gufis from train labels because they werent in etd')\n",
    "\n",
    "    # Create a dataframe to store the extracted features\n",
    "    df_final_labels_etd_exp = df_final_labels_etd.copy(deep=True)\n",
    "    df_final_labels_etd_exp = df_final_labels_etd_exp.drop_duplicates(subset=['gufi'])\n",
    "    new_planes_indexes_labels = list(df_final_labels_etd_exp.index)\n",
    "\n",
    "    # Process each unique plane and extract the estimated time of departure (ETD) features\n",
    "    curr_plane_counter = 1\n",
    "    list_dicts_etd = []\n",
    "    for index_counter, curr_train_labels_indx in tqdm(enumerate(new_planes_indexes_labels), total=len(new_planes_indexes_labels)):\n",
    "        if len(new_planes_indexes_labels) - 1 == index_counter:\n",
    "            break\n",
    "        df_etd_limited_plane = df_etd_limited.iloc[new_planes_indexes_etd[curr_plane_counter - 1]:new_planes_indexes_etd[curr_plane_counter]]\n",
    "        curr_gufi = df_final_labels_etd.gufi.iloc[curr_train_labels_indx]\n",
    "        list_timepoints = list(df_final_labels_etd.timestamp.iloc[curr_train_labels_indx:new_planes_indexes_labels[index_counter + 1]])\n",
    "        dep_times_dicts = extract_etdv3(curr_gufi, list_timepoints, df_etd_limited_plane)\n",
    "        list_dicts_etd.extend(dep_times_dicts)\n",
    "        curr_plane_counter += 1\n",
    "\n",
    "    # Save the extracted ETD features to a CSV file\n",
    "    time_etd_df = pd.DataFrame(list_dicts_etd)\n",
    "    etd_sav_path = f\"{sav_dir}timepointgufi_{airport}_etd.csv\"\n",
    "    print(f'Saving the following file: {etd_sav_path}')\n",
    "    time_etd_df.to_csv(etd_sav_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pushback_plane_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
