{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f6c34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Data manipulation and analysis library\n",
    "import numpy as np  # Numerical computing library\n",
    "import xgboost as xgb  # Gradient boosting library\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error  # Evaluation metrics\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit  # Data splitting functions\n",
    "from sklearn.tree import DecisionTreeRegressor  # Decision Tree Regressor model\n",
    "from sklearn.linear_model import LinearRegression  # Linear Regression model\n",
    "from sklearn.ensemble import RandomForestRegressor  # Random Forest Regressor model\n",
    "from sklearn.impute import SimpleImputer  # Imputation of missing values\n",
    "from pathlib import Path  # Object-oriented filesystem paths\n",
    "from typing import Any  # Type hinting\n",
    "from loguru import logger  # Logging library\n",
    "import pandas as pd  # Data manipulation and analysis library (imported twice)\n",
    "import pickle  # Object serialization and deserialization\n",
    "import csv  # Reading and writing CSV files\n",
    "import os  # Operating system interfaces\n",
    "import re  # Regular expression operations\n",
    "import math  # Mathematical functions\n",
    "import time  # Time access and conversions\n",
    "from tensorflow import keras  # Deep learning library\n",
    "from collections import deque  # Double-ended queue data structure\n",
    "from sklearn.metrics import roc_auc_score  # Evaluation metric for binary classification\n",
    "from xgboost import XGBClassifier  # XGBoost Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53a1ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model data will be saved at: Inference_Predictions//0\n"
     ]
    }
   ],
   "source": [
    "# Model and threshold parameters\n",
    "model_dir_path = \"Models/chosen\"\n",
    "threshold_ml_classifier_model = .50\n",
    "mae_thresh_bad = 30\n",
    "mae_thresh_good = 20\n",
    "bad_airports = [\"KDFW\", \"KJFK\", \"KMEM\", \"KMIA\"]\n",
    "good_airports = [\"KATL\", \"KCLT\", \"KDEN\", \"KORD\", \"KPHX\", \"KSEA\"]\n",
    "\n",
    "# Debug and model type\n",
    "debug = True\n",
    "comment = \"submission etd airlinecode taxitime\"\n",
    "model_type = \"xgb classifier\"\n",
    "\n",
    "# Directory paths for loading data\n",
    "raw_label_load_dir = \"Data/\"\n",
    "indiv_features_load_dir = f\"Inference_Extracted_Features/\"\n",
    "\n",
    "# Feature categories\n",
    "unique_timepoint_features = [\"taxitime_to_gate\"]\n",
    "unique_gufi_features = [\"airlinecode\"]\n",
    "unique_timepointgufi_features = [\"etd\"]\n",
    "\n",
    "# Root directories for feature categories\n",
    "timepoint_root = f\"{indiv_features_load_dir}timepoint\"\n",
    "gufi_root = f\"{indiv_features_load_dir}gufi\"\n",
    "timepointgufi_root = f\"{indiv_features_load_dir}timepointgufi\"\n",
    "\n",
    "# Create inference save directory\n",
    "inference_save_dir_root = f\"Inference_Predictions/\"\n",
    "inference_save_dir = f\"{inference_save_dir_root}/0/\"\n",
    "run_id = 0\n",
    "while os.path.isdir(inference_save_dir):\n",
    "    run_id = run_id + 1\n",
    "    inference_save_dir = f\"{inference_save_dir_root}{run_id}/\"\n",
    "os.mkdir(inference_save_dir)\n",
    "print(f\"Model data will be saved at: {inference_save_dir}\")\n",
    "\n",
    "# Filename for overall submission\n",
    "overall_prediction_file_name_submission = f\"{inference_save_dir}overall_submission.csv\"\n",
    "\n",
    "# Directory paths for loading submission data\n",
    "indiv_features_load_dir_submission = f\"Inference_Extracted_Features/Current_Features/\"\n",
    "timepoint_root_submission = f\"{indiv_features_load_dir_submission}timepoint\"\n",
    "gufi_root_submission = f\"{indiv_features_load_dir_submission}gufi\"\n",
    "timepointgufi_root_submission = f\"{indiv_features_load_dir_submission}timepointgufi\"\n",
    "\n",
    "# CSV header\n",
    "header = [\"gufi\", \"timestamp\", \"airport\", \"minutes_until_pushback\"]\n",
    "\n",
    "# Function to split GUFI and add airline_code, plane_id, departing_airport_code, and arriving_airport_code columns\n",
    "def split_gufi(curr_df):\n",
    "    try:\n",
    "        curr_df[['plane_id','departing_airport_code','arriving_airport_code']] =  curr_df.gufi.str.split('.', expand = True)[[0,1,2]]\n",
    "        curr_df['airline_code'] = curr_df.gufi.str[:3]\n",
    "    except:\n",
    "        logger.info(\"ERRROR IN SPLIT GUFI\")\n",
    "        logger.info(f\"{list(curr_df.gufi)}\")\n",
    "    return curr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2fcfb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 09:26:28.123 | INFO     | __main__:load_model:23 - Trying to load model from: Models/chosen/KATL_xgb classifier_NOOUTLIER_submission etd airlinecode taxitime.pkl\n",
      "2023-05-02 09:26:28.129 | INFO     | __main__:load_model:36 - Model loaded successfully\n",
      "2023-05-02 09:26:28.130 | INFO     | __main__:load_model:23 - Trying to load model from: Models/chosen/KCLT_xgb classifier_NOOUTLIER_submission etd airlinecode taxitime.pkl\n",
      "2023-05-02 09:26:28.136 | INFO     | __main__:load_model:36 - Model loaded successfully\n",
      "2023-05-02 09:26:28.136 | INFO     | __main__:load_model:23 - Trying to load model from: Models/chosen/KDEN_xgb classifier_NOOUTLIER_submission etd airlinecode taxitime.pkl\n",
      "2023-05-02 09:26:28.141 | INFO     | __main__:load_model:36 - Model loaded successfully\n",
      "2023-05-02 09:26:28.142 | INFO     | __main__:load_model:23 - Trying to load model from: Models/chosen/KDFW_xgb classifier_NOOUTLIER_submission etd airlinecode taxitime.pkl\n",
      "2023-05-02 09:26:28.145 | INFO     | __main__:load_model:36 - Model loaded successfully\n",
      "2023-05-02 09:26:28.146 | INFO     | __main__:load_model:23 - Trying to load model from: Models/chosen/KJFK_xgb classifier_NOOUTLIER_submission etd airlinecode taxitime.pkl\n",
      "2023-05-02 09:26:28.150 | INFO     | __main__:load_model:36 - Model loaded successfully\n",
      "2023-05-02 09:26:28.151 | INFO     | __main__:load_model:23 - Trying to load model from: Models/chosen/KMEM_xgb classifier_NOOUTLIER_submission etd airlinecode taxitime.pkl\n",
      "2023-05-02 09:26:28.154 | INFO     | __main__:load_model:36 - Model loaded successfully\n",
      "2023-05-02 09:26:28.155 | INFO     | __main__:load_model:23 - Trying to load model from: Models/chosen/KMIA_xgb classifier_NOOUTLIER_submission etd airlinecode taxitime.pkl\n",
      "2023-05-02 09:26:28.159 | INFO     | __main__:load_model:36 - Model loaded successfully\n",
      "2023-05-02 09:26:28.159 | INFO     | __main__:load_model:23 - Trying to load model from: Models/chosen/KORD_xgb classifier_NOOUTLIER_submission etd airlinecode taxitime.pkl\n",
      "2023-05-02 09:26:28.163 | INFO     | __main__:load_model:36 - Model loaded successfully\n",
      "2023-05-02 09:26:28.164 | INFO     | __main__:load_model:23 - Trying to load model from: Models/chosen/KPHX_xgb classifier_NOOUTLIER_submission etd airlinecode taxitime.pkl\n",
      "2023-05-02 09:26:28.168 | INFO     | __main__:load_model:36 - Model loaded successfully\n",
      "2023-05-02 09:26:28.168 | INFO     | __main__:load_model:23 - Trying to load model from: Models/chosen/KSEA_xgb classifier_NOOUTLIER_submission etd airlinecode taxitime.pkl\n",
      "2023-05-02 09:26:28.171 | INFO     | __main__:load_model:36 - Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "def load_model(solution_directory):\n",
    "    \"\"\"\n",
    "    Load any model assets from disk.\n",
    "    \n",
    "    Args:\n",
    "        solution_directory (str): Path to the directory containing the saved models.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the loaded models for each airport.\n",
    "    \"\"\"\n",
    "    # List of airports\n",
    "    airports = ['KATL', 'KCLT', 'KDEN', 'KDFW', 'KJFK', 'KMEM', 'KMIA', 'KORD', 'KPHX', 'KSEA']\n",
    "    \n",
    "    # Initialize an empty dictionary to store the loaded models\n",
    "    model = {}\n",
    "    \n",
    "    for curr_airport in airports:\n",
    "        # Initialize an empty dictionary for the current airport's models\n",
    "        airport_models = {}\n",
    "        \n",
    "        # Load the regressor model\n",
    "        model_path = f'{solution_directory}/{curr_airport}_xgb classifier_NOOUTLIER_submission etd airlinecode taxitime.pkl'\n",
    "        logger.info(f\"Trying to load model from: {model_path}\")\n",
    "        airport_models['regressor'] = pickle.load(open(model_path, 'rb'))\n",
    "        \n",
    "        # Load the classifier model\n",
    "        model_path = f'{solution_directory}/{curr_airport}_estimation_classifier.pkl'\n",
    "        airport_models['classifier'] = pickle.load(open(model_path, 'rb'))\n",
    "        \n",
    "        # Load the classifier parameters\n",
    "        model_path = f'{solution_directory}/{curr_airport}_estimation_parameters.pkl'\n",
    "        airport_models['classifier_params'] = pickle.load(open(model_path, 'rb'))\n",
    "        \n",
    "        # Store the loaded models for the current airport\n",
    "        model[curr_airport] = airport_models\n",
    "        logger.info(f\"Model loaded successfully\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load the models from the specified directory\n",
    "model = load_model(model_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5d465bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Doing airport: KATL\n",
      "Loaded etd features from from : Inference_Extracted_Features/Current_Features/timepointgufi_KATL_etd.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 09:27:17.824 | INFO     | __main__:<module>:87 - Trying now to make the df to feed into predict\n",
      "2023-05-02 09:27:17.825 | INFO     | __main__:<module>:88 - Missing these columns: set()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded taxitime to gate features from from : Inference_Extracted_Features/Current_Features/timepoint_KATL_taxitime_to_gate.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 09:27:18.038 | INFO     | __main__:<module>:109 - This len df is returned from the predict method: 303836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Doing airport: KCLT\n",
      "Loaded etd features from from : Inference_Extracted_Features/Current_Features/timepointgufi_KCLT_etd.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 09:27:20.663 | INFO     | __main__:<module>:87 - Trying now to make the df to feed into predict\n",
      "2023-05-02 09:27:20.663 | INFO     | __main__:<module>:88 - Missing these columns: set()\n",
      "2023-05-02 09:27:20.818 | INFO     | __main__:<module>:109 - This len df is returned from the predict method: 198963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded taxitime to gate features from from : Inference_Extracted_Features/Current_Features/timepoint_KCLT_taxitime_to_gate.csv\n",
      "-----------------------------\n",
      "Doing airport: KDEN\n",
      "Loaded etd features from from : Inference_Extracted_Features/Current_Features/timepointgufi_KDEN_etd.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 09:27:23.056 | INFO     | __main__:<module>:87 - Trying now to make the df to feed into predict\n",
      "2023-05-02 09:27:23.057 | INFO     | __main__:<module>:88 - Missing these columns: set()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded taxitime to gate features from from : Inference_Extracted_Features/Current_Features/timepoint_KDEN_taxitime_to_gate.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 09:27:23.263 | INFO     | __main__:<module>:109 - This len df is returned from the predict method: 281311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Doing airport: KDFW\n",
      "Loaded etd features from from : Inference_Extracted_Features/Current_Features/timepointgufi_KDFW_etd.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 09:27:25.893 | INFO     | __main__:<module>:87 - Trying now to make the df to feed into predict\n",
      "2023-05-02 09:27:25.893 | INFO     | __main__:<module>:88 - Missing these columns: set()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded taxitime to gate features from from : Inference_Extracted_Features/Current_Features/timepoint_KDFW_taxitime_to_gate.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 09:27:26.142 | INFO     | __main__:<module>:109 - This len df is returned from the predict method: 297171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Doing airport: KJFK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 09:27:28.628 | INFO     | __main__:<module>:87 - Trying now to make the df to feed into predict\n",
      "2023-05-02 09:27:28.629 | INFO     | __main__:<module>:88 - Missing these columns: set()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded etd features from from : Inference_Extracted_Features/Current_Features/timepointgufi_KJFK_etd.csv\n",
      "Loaded taxitime to gate features from from : Inference_Extracted_Features/Current_Features/timepoint_KJFK_taxitime_to_gate.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 09:27:28.720 | INFO     | __main__:<module>:109 - This len df is returned from the predict method: 99604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Doing airport: KMEM\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m airlinecode_features \u001b[39m=\u001b[39m [curr_feat \u001b[39mfor\u001b[39;00m curr_feat \u001b[39min\u001b[39;00m all_trained_features \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(curr_feat) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m]\n\u001b[1;32m     51\u001b[0m \u001b[39m####SUBMISSION\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m df_predict \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mraw_label_load_dir\u001b[39m}\u001b[39;49;00m\u001b[39msubmission_data.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     53\u001b[0m \u001b[39m# Convert timestampe to pandas datetime\u001b[39;00m\n\u001b[1;32m     54\u001b[0m df_predict[\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df_predict[\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m])   \n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pushback_plane_tf/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pushback_plane_tf/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pushback_plane_tf/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pushback_plane_tf/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pushback_plane_tf/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m         nrows\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pushback_plane_tf/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pushback_plane_tf/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pushback_plane_tf/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pushback_plane_tf/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pushback_plane_tf/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pushback_plane_tf/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1158\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pushback_plane_tf/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[39m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m     \u001b[39m#  here too.\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m     \u001b[39m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m     \u001b[39m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1430\u001b[0m     )\n\u001b[0;32m-> 1433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m   1434\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m \u001b[39m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(arr_or_dtype, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def grab_airlinecodes(df_predict, airlinecode_features):\n",
    "    \"\"\"\n",
    "    Grab airline codes from the given dataframe and perform one-hot encoding.\n",
    "    \n",
    "    Args:\n",
    "        df_predict (pd.DataFrame): DataFrame containing the prediction data.\n",
    "        airlinecode_features (list): List of airline codes.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with one-hot encoded airline codes.\n",
    "    \"\"\"\n",
    "    df_predict_copy = df_predict[['gufi']].copy(deep = True)\n",
    "    df_predict_copy = df_predict_copy.drop_duplicates(subset=\"gufi\")\n",
    "    df_predict_copy = split_gufi(df_predict_copy)\n",
    "    airlinecode_features_copy = airlinecode_features\n",
    "    if \"Other\" in airlinecode_features_copy:\n",
    "        airlinecode_features_copy.remove(\"Other\")\n",
    "    airlines_to_keep = airlinecode_features_copy\n",
    "    df_predict_copy['airline_code'] = np.where(df_predict_copy['airline_code'].isin(airlines_to_keep), df_predict_copy['airline_code'], 'Other')\n",
    "    one_hot_encoded = pd.get_dummies(df_predict_copy['airline_code'])\n",
    "    df_predict_copy = pd.concat([df_predict_copy, one_hot_encoded], axis=1)        \n",
    "    #df_predict = df_predict.drop(columns=['airport'])\n",
    "    df_predict = pd.merge(df_predict, df_predict_copy, on=['gufi'], how=\"left\")\n",
    "    airlinecode_features.append(\"Other\")\n",
    "    for curr_airline in airlinecode_features:\n",
    "        if curr_airline not in df_predict.columns:\n",
    "            df_predict[curr_airline] = 0\n",
    "    return df_predict\n",
    "\n",
    "\n",
    "# Load the list of airports\n",
    "list_airports = [\"KATL\", \"KCLT\", \"KDEN\", \"KDFW\", \"KJFK\", \"KMEM\", \"KMIA\", \"KORD\", \"KPHX\", \"KSEA\"]\n",
    "\n",
    "# Iterate through each airport to make predictions\n",
    "for airport in list_airports:\n",
    "    print(f'-----------------------------')\n",
    "    print(f'Doing airport: {airport}')\n",
    "    curr_model_regression_lower = model[f'{airport}']['regressor']\n",
    "    estimate_classifier = model[f'{airport}']['classifier']\n",
    "    estimate_classifier_params = model[f'{airport}']['classifier_params']\n",
    "\n",
    "    # Load data and preprocess it\n",
    "    # 1. Load submission data\n",
    "    # 2. Convert timestamp to pandas datetime\n",
    "    # 3. Filter data for the current airport\n",
    "\n",
    "    #getting the airline code of the new planes\n",
    "    all_trained_features = list(curr_model_regression_lower.get_booster().feature_names)\n",
    "    airlinecode_features = [curr_feat for curr_feat in all_trained_features if len(curr_feat) == 3]\n",
    "\n",
    "    ####SUBMISSION\n",
    "    df_predict = pd.read_csv(f\"{raw_label_load_dir}submission_data.csv\")\n",
    "    # Convert timestampe to pandas datetime\n",
    "    df_predict['timestamp'] = pd.to_datetime(df_predict['timestamp'])   \n",
    "    df_predict = df_predict[df_predict.airport == airport]\n",
    "\n",
    "\n",
    "    feature_cols = ['unix_time']\n",
    "    ###ETD\n",
    "    etd_file_path = f\"{timepointgufi_root_submission}_{airport}_etd.csv\"\n",
    "    df_etd = pd.read_csv(etd_file_path, parse_dates=[\"timestamp\"])\n",
    "    print(f\"Loaded etd features from from : {etd_file_path}\")\n",
    "    df_predict = pd.merge(df_predict, df_etd, on=['gufi', 'timestamp'], how=\"left\")\n",
    "\n",
    "\n",
    "    # Merge airline code features\n",
    "    df_predict = grab_airlinecodes(df_predict, airlinecode_features)\n",
    "\n",
    "    ###taxitimetogate\n",
    "    taxitime_to_gate_file_path = f\"{timepoint_root_submission}_{airport}_taxitime_to_gate.csv\"\n",
    "    df_taxitime_to_gate = pd.read_csv(taxitime_to_gate_file_path, parse_dates=[\"timestamp\"])\n",
    "    print(f\"Loaded taxitime to gate features from from : {taxitime_to_gate_file_path}\")\n",
    "    df_predict = pd.merge(df_predict, df_taxitime_to_gate, on=['timestamp'], how=\"left\")\n",
    "    taxitime_to_gate_features = list(df_taxitime_to_gate.columns)\n",
    "    if \"Unnamed: 0\" in list(taxitime_to_gate_features):\n",
    "        taxitime_to_gate_features.remove(\"Unnamed: 0\")\n",
    "    taxitime_to_gate_features.remove('found_counts_taxitime_to_gate')\n",
    "    taxitime_to_gate_features.remove('timestamp')\n",
    "\n",
    "    if \"Unnamed: 0\" in list(df_predict):\n",
    "        df_predict.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    # extract year, month, day, and hour information\n",
    "    df_predict['unix_time'] = df_predict['timestamp'].astype(np.int64)  // 10**9\n",
    "    missing_feats = set(all_trained_features) - set(list(df_predict.columns))\n",
    "    if debug:\n",
    "        logger.info(\"Trying now to make the df to feed into predict\")\n",
    "        logger.info(f'Missing these columns: {missing_feats}')\n",
    "    if len(missing_feats) != 0:\n",
    "        logger.info(f'Missing these columns: {missing_feats}')\n",
    "    \n",
    "\n",
    "    y_pred_lower = curr_model_regression_lower.predict(df_predict[all_trained_features]) \n",
    "    \n",
    "    \n",
    "    median_underestimation = estimate_classifier_params['median_underestimation']\n",
    "    median_overestimation = estimate_classifier_params['median_overestimation']\n",
    "    X_test_lower = df_predict.copy(deep=True)\n",
    "    X_test_lower['pred_minutes_until_pushback'] = y_pred_lower\n",
    "    y_prob_estimate = estimate_classifier.predict_proba(X_test_lower[all_trained_features + ['pred_minutes_until_pushback']])[:, 1]\n",
    "    X_test_lower = X_test_lower.reset_index(drop=True)\n",
    "    X_test_lower['final_pred_minutes_until_pushback'] = np.where(y_prob_estimate > 0.5, X_test_lower['pred_minutes_until_pushback'] + median_underestimation, X_test_lower['pred_minutes_until_pushback'])\n",
    "    X_test_lower['final_pred_minutes_until_pushback'] = np.where(y_prob_estimate < 0.5, X_test_lower['final_pred_minutes_until_pushback'] - median_overestimation, X_test_lower['final_pred_minutes_until_pushback'])\n",
    "    y_pred = X_test_lower['final_pred_minutes_until_pushback'].values\n",
    "\n",
    "    df_predict['minutes_until_pushback'] = np.int32(np.around(y_pred,decimals=0))   \n",
    "\n",
    "    if debug:\n",
    "        logger.info(f\"This len df is returned from the predict method: {len(df_predict)}\")        \n",
    "    \n",
    "    df_predict = df_predict.reset_index(drop=True)\n",
    "\n",
    "    prediction_file_name_submission = f\"{inference_save_dir}{airport}_submission.csv\"\n",
    "    df_predict[[\"gufi\",\"timestamp\",\"airport\",\"minutes_until_pushback\"]].to_csv(prediction_file_name_submission, index = 0)\n",
    "\n",
    "    df_predict[[\"gufi\",\"timestamp\",\"airport\",\"minutes_until_pushback\"]].to_csv(overall_prediction_file_name_submission, index = 0, mode='a', header = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pushback_plane_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
