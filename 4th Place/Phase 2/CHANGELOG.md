# Changelog
## [1.0.1] - 2023-08-21

### Refactored
- **Breaking:**: Simplified working with different variations of code through refactoring, such as specifying which version of the model to work with through adding new arguments, because our first generating script would misplace training data, which previously caused our model to generalize poorly. 

### Added
- Extended feature set to include additional features, working from an assumption that poor generalization was caused by lack of data
- Simplified overall pytorch model net architecture because of hardware limitations. 
- Added new arguments to main, see ReadMe for additional specification of appropriate usage.


### New MAE results
New MAE was much lower (21), however appeared to be more consistent with model's true performance on unseen data. However, the bad generalization still presist [in both centralized and federated approaches, more pronounced in federated approach]. When the tree-based learning was utilized, the accuracy was much better. The conclusion we have is that due to a stochastic nature of stochastic gradient descent, the model cannot generalize well. We noticed a better, but still extremely poor generalization of Centralized 3-layer MLP with the 32,64,64 layer architecture, but low accuracy was displayed with both centralized (accuracy of 29) and Federated Model (claimed validation accuracy of 21, average training loss of 15), potentially due to our model small structure. However, we could not adjust it significantly due to particular available hardware limitations. The fact that apparent validation accuracy decreased significantly after utilizing bigger dataset, and probably is approaching its true inference accuracy, was a strong evidence supporting the hypothesis that our particular architecture, or MLP in general, was potentially not be the most appropriate model for a given size of training dataset, for even in comparison of Centralized MLP and Centralized Tree-based structure, we found tree-based models were outperforming MLP rather frequently (even though some evaluations showed a comparable overall accuracy at times when the epoch number of MLP training was increased to significantly higher number, such as 50, which takes certain high resources and time to compute in Federated Manner, causing us to decrease the epoch size) by a significant margin (estimated 12 MAE based on local evaluation of LightGBM model vs 29 MAE on same local validation dataset of Centralized MLP occasionally), which could be explained by tree's natural ability to fit better with a smaller amount of training data. Our team also supposed that NN's poor generalization was affected by that and various other factors, such as maybe insufficient amount of epochs for training, however, that could not be empirically proven within the limits of tested training epochs. Therefore, our team thought that changing clients' number of epochs may allow to achive a significantly more advanced ability in the federated aggregated model to generalize over the various possible datasets, potentially achiving results comparable to tree-based approaches, as our local centralized evaluation comparison of models showed us. We concluded that our local analysis could potentially hint that MLP has significant potential to perform well, however requires rather high computational resources. 
